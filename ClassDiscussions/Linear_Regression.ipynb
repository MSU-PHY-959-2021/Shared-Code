{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some data\n",
    "\n",
    "We saw a linear classification problem that did NOT work out.  To make life a bit easier, let's try a linear regression problem and then extend what we learned to a linear classification problem that is more or less linearly separable.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = 100 # number of points per class\n",
    "\n",
    "X = np.linspace(0.0,1,N) \n",
    "y = np.linspace(0.0,1,N) \n",
    "\n",
    "for j in range(N):\n",
    "  yy = 1.1*X[j]+np.random.randn()*0.2-0.1\n",
    "  y[j] = yy\n",
    "\n",
    "plt.scatter(X, y, c=\"red\", s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,1.5])\n",
    "#fig.savefig('spiral_raw.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's perform linear regression on this data\n",
    "\n",
    "This is a pretty straight forward example!  We want a linear function with two variables\n",
    "\n",
    "$f(X) = W_1 \\cdot X_1 + b$\n",
    "\n",
    "Here W is a weight and b is what's known as a bias variable.  You can just think of these as the slope and offset.  We also have the \"targets\" or truth labels for the data.  This is just the variable on the y-axis.\n",
    "\n",
    "Let's do this \"by hand\" using mean squared error first, using gradient descent.  Then we can revisit the problem to see if we get the \"right\" answer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear regression\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.5\n",
    "b = 0\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 0.5  # why not make this 1.0?\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "Niter = 200\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(Niter):\n",
    "  \n",
    "    # evaluate function values\n",
    "    # fhat = W^TX +b\n",
    "    fhat = W*X + b \n",
    "    \n",
    "    # compute mean squared loss\n",
    "    data_loss = np.sum((y-fhat)**2)\n",
    "    data_loss /= N\n",
    "    \n",
    "    # This is L2 regularization, see Lecture 5\n",
    "    reg_loss = 0.5*reg*W*W\n",
    "    \n",
    "    # Total loss is the sum\n",
    "    loss = reg_loss + data_loss\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "        \n",
    "    #compute the loss gradients\n",
    "    dW = -2*np.sum(X*(y-fhat))\n",
    "    db = -2*np.sum(y-fhat)\n",
    "    dW /= N\n",
    "    db /= N\n",
    "    \n",
    "    dW += reg*W # regularization gradient\n",
    "    \n",
    "    W -= step_size*dW\n",
    "    b -= step_size*db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make it clear what we're doing, let's peek at the weights\n",
    "print(\"Slope:\\n\",W)\n",
    "print(\"\\nOffset:\\n\",b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection, part 1:\n",
    "\n",
    "OK, that's pretty close to waht we expected!  We should have found slope = 1.1 and offset = -0.1.  But the noise in the problem will definitely give us some stochasticity in our results.  We should always make plots to validate that we're getting what we expect.  Let's do that here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot our data to see if it makes sense!\n",
    "plt.scatter(X, y, c=\"red\", s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,1.5])\n",
    "\n",
    "line = X*W + b\n",
    "plt.plot(X,line,color=\"blue\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make a plot of the residuals!\n",
    "# Here we use a useful package called seaborn\n",
    "# This effectively subtracts the prediction from the data\n",
    "# Then it does a fit of a polynomial to the data\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.residplot(x=line, y=y, lowess=True, color=\"b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection, part 2:\n",
    "\n",
    "That was a fair amount of code to write for a simple linear regression.  If you recall the lecture slides, you'll note that in the case of mean square error loss we have an analytic solution.  On your homework you'll get the chance to use linear algebra to solve this problem in <5 lines!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving on to linear classification!\n",
    "\n",
    "Now let's \"invent\" some data that should be more or less linearly separable.  In this way we can now revisit our approach that gave us a very marginal linear classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "\n",
    "N = 150 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "\n",
    "slopeDiff = 0\n",
    "offsetDiff = 1.0\n",
    "\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  xx = np.linspace(0,1,N) \n",
    "  yy = (1+slopeDiff*j)*xx + np.random.randn(N)*(0.2+j*0.05) + j*offsetDiff \n",
    "  X[ix] = np.c_[xx,yy]\n",
    "  y[ix] = j\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "Niter = 200\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(Niter):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # fhat = W^TX +b\n",
    "    scores = np.dot(X, W) + b \n",
    "      \n",
    "    # compute the class probabilities\n",
    "    # this is softmax, converting to class probability\n",
    "    # prob1 = exp(score1)/{exp(score1)+exp(score2)+...}\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "      \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    # sometimes cross-entropy is called \"log loss\"\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "\n",
    "    # This is L2 regularization, see Lecture 5\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    \n",
    "    # Total loss is distance between the line and the data point\n",
    "    # plus the L2 reg loss\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "    dW += reg*W # regularization gradient\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make it clear what we're doing, let's peek at the weights\n",
    "print(\"Weight matrix:\\n\",W)\n",
    "print(\"\\nBias matrix:\\n\",b)\n",
    "\n",
    "\n",
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('\\n training accuracy: %.2f' % (np.mean(predicted_class == y)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look to see what we got!\n",
    "# Our 2D weights/variables define a plane\n",
    "\n",
    "#make some x-values\n",
    "xyMnx = 1.2\n",
    "X_test = np.linspace(-0.5,1.5,N)\n",
    "Y_test = np.linspace(-0.5,4,N)\n",
    "\n",
    "Xt, Yt = np.meshgrid(X_test, Y_test)\n",
    "\n",
    "#2D plane separating class 1 from class 2\n",
    "Z1 = Xt*W[0,0] + Yt*W[1,0] + b[0,0]\n",
    "\n",
    "\n",
    "#this line follows the Z=0 contour.  How did we find that??\n",
    "line1 = (W[0,0]*X + b[0,0])/(-1*W[1,0])\n",
    "\n",
    "\n",
    "cp = plt.contourf(Xt, Yt, Z1,10, cmap=plt.cm.plasma, alpha=0.9)\n",
    "plt.colorbar(cp)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.plot(X,line1,color=\"blue\")\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some comparisons!\n",
    "# This line should be exactly between the populations in space\n",
    "line2 = (1+slopeDiff*0.5)*X + offsetDiff*0.5\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Why is line1 not equal to line2??\n",
    "plt.plot(X,line2,color='r',label=\"Equidistant\")\n",
    "plt.plot(X,line1,color='b',label=\"Linear Regression\")\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "hand, labl = ax.get_legend_handles_labels()\n",
    "handout = [hand[0],hand[2]]\n",
    "lablout = [labl[0],labl[2]]\n",
    "plt.legend(handout,lablout)\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we'd really like, though, is to classify space where\n",
    "# each class score has the highest probability.\n",
    "# This divides the space into Voronoi regions\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally, let's take a look at what we're really doing here.\n",
    "This linear classifier gives us a way to map a position in 2D space into a 1D space with \"as optimal as possible\" separation between classes.  We can envision this by using our function $f(X) = W^TX+b$ to calculate a single test statistic value for each entry/event.  We have three classes and can now just histogram each class to see the separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have N x 3 and need the first column split by \n",
    "Xplt = np.zeros((N,3))\n",
    "Xplt[:,0] = scores[:N,0]\n",
    "Xplt[:,1] = scores[N:2*N,0]\n",
    "Xplt[:,2] = scores[2*N:3*N,0]\n",
    "\n",
    "plt.hist(Xplt,30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
