{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some data\n",
    "\n",
    "We want a problem that is not linearly separable to show how poorly a linear classification would work here.\n",
    "\n",
    "Note later that centering at zero is helpful for us.  We will make three classes, but you could add more or less if you wish by adjusting the variable K.  Feel free to add more dimensions, but it's harder to view beyond 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = 150 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "\n",
    "easy = 0  #set this to 1/True to have an easier data sample to train on\n",
    "slopeDiff = 0.0\n",
    "offsetDiff = 0.3\n",
    "\n",
    "min = -1\n",
    "max = 1\n",
    "if easy:\n",
    "    min = -0.5\n",
    "    max = 2\n",
    "    \n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "    xx = r*np.sin(t)\n",
    "    yy = r*np.cos(t)\n",
    "    if easy:\n",
    "        xx = np.linspace(0,1,N)\n",
    "        yy = (0.5+0.5*slopeDiff*(j+1))*xx + np.random.randn(N)*(0.05+j*0.05) + j*offsetDiff\n",
    "    \n",
    "    X[ix] = np.c_[xx,yy]\n",
    "    y[ix] = j\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([min,max])\n",
    "plt.ylim([min,max])\n",
    "#fig.savefig('spiral_raw.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a logistic regression classifier\n",
    "\n",
    "We saw that we can perform linear regression on this data sample, so this is a good opportunity to try out logistic regression.  We can use more or less the same approach as before.  In logistic regression, we're going to be trying to model the output of our function as 0 (eg, low probability to match our data) vs 1 (eg, high probability to match our data).\n",
    "\n",
    "\n",
    "This time we will use an activation function to pass our linear model into.  This should not change things drastically, but we can compare and contrast the result from Linear regression.\n",
    "\n",
    "But we need to study some activation functions first.  Let's see what we can find out about the following options:\n",
    "  * Rectified Logical Unit (ReLU)\n",
    "  * Sigmoid\n",
    "  * SoftPlus\n",
    "  * Leaky ReLU\n",
    "  * Exponential ReLU (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our NN activation function here\n",
    "def activation(i, input):\n",
    "\n",
    "    if i == 0:\n",
    "        # Rectified linear unit (ReLU) activation\n",
    "        output = np.maximum(0,input)\n",
    "    elif i == 1:\n",
    "        #Sigmoid\n",
    "        output =  1/(1+np.exp(-1*input))\n",
    "    elif i == 2:\n",
    "        # SoftPlus\n",
    "        output = np.log(1+np.exp(input))\n",
    "    elif i == 3:\n",
    "        # Leaky ReLU\n",
    "        output = np.maximum(0.1*input,0.9*input)\n",
    "    elif i == 4:\n",
    "        # Exponential ReLU\n",
    "        output = np.where(input>0,input,np.exp(input)-1)\n",
    "    else:\n",
    "        return input\n",
    "    \n",
    "    return output\n",
    "\n",
    "ix = np.linspace(-4,4,200)\n",
    "\n",
    "plt.plot(ix,activation(0,ix),color=\"red\",label=\"ReLU\")\n",
    "plt.plot(ix,activation(1,ix),color=\"blue\",label=\"Sigmoid\")\n",
    "plt.plot(ix,activation(2,ix),color=\"green\",label=\"SoftPlus\")\n",
    "plt.plot(ix,activation(3,ix),color=\"magenta\",label=\"Leaky ReLU\")\n",
    "plt.plot(ix,activation(4,ix),color=\"orange\",label=\"ELU\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(iAct,X,W,b):\n",
    "    fhat = np.dot(X, W) + b \n",
    "    scores = activation(iAct,fhat)\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "iAct = 0 # choice of activation function\n",
    "\n",
    "#Need to threshold our classes!\n",
    "aThresh = 1e-2\n",
    "if (iAct == 1) or (iAct == 2):\n",
    "    aThresh = 0.45\n",
    "    \n",
    "Niter = 2000\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "for i in range(Niter):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # fhat = W^TX +b\n",
    "    probs = eval(iAct,X,W,b)\n",
    "    \n",
    "    #predicted_class = np.int64(scores[:,0]>aThresh)\n",
    "\n",
    "    # compute the loss: \n",
    "#    data_loss = np.square(predicted_class - y).sum() / num_examples\n",
    "\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    \n",
    "    # This is L2 regularization, see Lecture 5\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "   \n",
    "    # Total loss is distance between the line and the data point\n",
    "    # plus the L2 reg loss\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "    \n",
    "    # compute the gradient on scores\n",
    "#    dscores = 2.0 * (scores - yy)\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "    \n",
    "    \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    dW += reg*W # regularization gradient\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make it clear what we're doing, let's peek at the weights\n",
    "print(\"Weight matrix:\\n\",W)\n",
    "print(\"\\nBias matrix:\\n\",b)\n",
    "\n",
    "\n",
    "# evaluate training set accuracy\n",
    "scores = eval(iAct,X,W,b)\n",
    "\n",
    "#predicted_class = np.int64(scores[:,0]>aThresh)\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "\n",
    "\n",
    "print('\\n training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.ones(N)\n",
    "X_test *= 0.5\n",
    "Y_test = np.linspace(-0.5,2.5,N)\n",
    "\n",
    "line = activation(iAct,X_test*W[0,0] + Y_test*W[1,0] + b[0,0])\n",
    "\n",
    "plt.plot(Y_test,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look to see what we got!\n",
    "# Our 2D weights/variables define a plane\n",
    "\n",
    "#make some x-values\n",
    "X_test = np.linspace(min,max,N)\n",
    "Y_test = np.linspace(min,max,N)\n",
    "Xt, Yt = np.meshgrid(X_test, Y_test)\n",
    "\n",
    "\n",
    "#2D plane separating class 1 from class 2\n",
    "Z1 = activation(iAct,Xt*W[0,0] + Yt*W[1,0] + b[0,0])\n",
    "Z2 = activation(iAct,Xt*W[0,1] + Yt*W[1,1] + b[0,1])\n",
    "Z3 = activation(iAct,Xt*W[0,2] + Yt*W[1,2] + b[0,2])\n",
    "\n",
    "\n",
    "cp = plt.contourf(Xt, Yt, Z1,100, cmap=plt.cm.Wistia, alpha=0.9)\n",
    "plt.colorbar(cp)\n",
    "plt.contour(Xt,Yt,Z1,[1])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.xlim([min,max])\n",
    "plt.ylim([min,max])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we'd really like, though, is to classify space where\n",
    "# each class score has the highest probability.\n",
    "# This divides the space into class regions\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = eval(iAct,np.c_[xx.ravel(), yy.ravel()], W , b)\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have N x K scores  \n",
    "# These are \"one vs all\" class scores\n",
    "# Eg, kClass=2 is class 2 vs the others\n",
    "\n",
    "kClass = 0\n",
    "Xplt = np.zeros((N,K))\n",
    "\n",
    "for j in range(K): \n",
    "    Xplt[:,j] = probs[j*N:(j+1)*N,kClass]\n",
    "\n",
    "plt.hist(Xplt,30)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
