{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some data\n",
    "\n",
    "We want a problem that is not linearly separable to show how poorly a linear classification would work here.\n",
    "\n",
    "Note later that centering at zero is helpful for us.  We will make three classes, but you could add more or less if you wish by adjusting the variable K.  Feel free to add more dimensions, but it's harder to view beyond 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(20)\n",
    "\n",
    "N = 150 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 2 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "\n",
    "slopeDiff = 1\n",
    "offsetDiff = 0.25\n",
    "\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  xx = np.linspace(0,1,N) \n",
    "  yy = (1+slopeDiff*j)*xx + np.random.randn(N)*(0.2+j*0.05) + j*offsetDiff \n",
    "  X[ix] = np.c_[xx,yy]\n",
    "  y[ix] = j\n",
    "\n",
    "# make a copy to help calculations later on\n",
    "yy = np.zeros((N*K,D))   \n",
    "\n",
    "yy[:,0] = y\n",
    "yy[:,1] = y\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a logistic regression classifier\n",
    "\n",
    "We saw that we can perform linear regression on this data sample, so this is a good opportunity to try out logistic regression.  We can use more or less the same approach as before.  In logistic regression, we're going to be trying to model the output of our function as 0 (eg, low probability to match our data) vs 1 (eg, high probability to match our data).\n",
    "\n",
    "\n",
    "This time we will use an activation function to pass our linear model into.  This should not change things drastically, but we can compare and contrast the result from Linear regression.\n",
    "\n",
    "But we need to study some activation functions first.  Let's see what we can find out about the following options:\n",
    "  * Rectified Logical Unit (ReLU)\n",
    "  * Sigmoid\n",
    "  * SoftPlus\n",
    "  * Leaky ReLU\n",
    "  * Exponential ReLU (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our NN activation function here\n",
    "def activation(i, input):\n",
    "\n",
    "    if i == 0:\n",
    "        # Rectified linear unit (ReLU) activation\n",
    "        output = np.maximum(0,input)\n",
    "    elif i == 1:\n",
    "        #Sigmoid\n",
    "        output =  1/(1+np.exp(-1*input))\n",
    "    elif i == 2:\n",
    "        # SoftPlus\n",
    "        output = np.log(1+np.exp(input))\n",
    "    elif i == 3:\n",
    "        # Leaky ReLU\n",
    "        output = np.maximum(0.1*input,0.9*input)\n",
    "    elif i == 4:\n",
    "        # Exponential ReLU\n",
    "        output = np.where(input>0,input,np.exp(input)-1)\n",
    "    else:\n",
    "        return input\n",
    "    \n",
    "    return output\n",
    "\n",
    "ix = np.linspace(-4,4,200)\n",
    "\n",
    "plt.plot(ix,activation(0,ix),color=\"red\",label=\"ReLU\")\n",
    "plt.plot(ix,activation(1,ix),color=\"blue\",label=\"Sigmoid\")\n",
    "plt.plot(ix,activation(2,ix),color=\"green\",label=\"SoftPlus\")\n",
    "plt.plot(ix,activation(3,ix),color=\"magenta\",label=\"Leaky ReLU\")\n",
    "plt.plot(ix,activation(4,ix),color=\"orange\",label=\"ELU\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 0.1\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "iAct = 0 # choice of activation function\n",
    "\n",
    "#Need to threshold our classes!\n",
    "aThresh = 1e-2\n",
    "if (iAct == 1) or (iAct == 2):\n",
    "    aThresh = 0.5\n",
    "    \n",
    "Niter = 2000\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "\n",
    "for i in range(Niter):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # fhat = W^TX +b\n",
    "    fhat = np.dot(X, W) + b \n",
    "    \n",
    "    scores = activation(iAct,fhat)\n",
    "\n",
    "    predicted_class = np.int64(scores[:,0]>aThresh)\n",
    "\n",
    "    # compute the loss: \n",
    "    data_loss = np.square(predicted_class - y).sum() / num_examples\n",
    "\n",
    "    # This is L2 regularization, see Lecture 5\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "   \n",
    "    # Total loss is distance between the line and the data point\n",
    "    # plus the L2 reg loss\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "    \n",
    "    # compute the gradient on scores\n",
    "    dscores = 2.0 * (scores - yy)\n",
    "    dscores /= num_examples\n",
    "    \n",
    "    \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "    \n",
    "    dW += reg*W # regularization gradient\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make it clear what we're doing, let's peek at the weights\n",
    "print(\"Weight matrix:\\n\",W)\n",
    "print(\"\\nBias matrix:\\n\",b)\n",
    "\n",
    "\n",
    "# evaluate training set accuracy\n",
    "scores = activation(iAct,np.dot(X, W) + b)\n",
    "predicted_class = np.int64(scores[:,0]>aThresh)\n",
    "print(predicted_class)\n",
    "print('\\n training accuracy: %.2f' % (np.mean(predicted_class == y)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.ones(N)\n",
    "X_test *= 0.5\n",
    "Y_test = np.linspace(-0.5,2.5,N)\n",
    "\n",
    "line = activation(iAct,X_test*W[0,0] + Y_test*W[1,0] + b[0,0])\n",
    "\n",
    "plt.plot(Y_test,line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look to see what we got!\n",
    "# Our 2D weights/variables define a plane\n",
    "\n",
    "#make some x-values\n",
    "X_test = np.linspace(-0.5,1.5,N)\n",
    "Y_test = np.linspace(-0.5,2.5,N)\n",
    "Xt, Yt = np.meshgrid(X_test, Y_test)\n",
    "\n",
    "\n",
    "#2D plane separating class 1 from class 2\n",
    "Z1 = activation(iAct,Xt*W[0,0] + Yt*W[1,0] + b[0,0])\n",
    "\n",
    "cp = plt.contourf(Xt, Yt, Z1,100, cmap=plt.cm.Wistia, alpha=0.9)\n",
    "plt.colorbar(cp)\n",
    "plt.contour(Xt,Yt,Z1,[aThresh])\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "\n",
    "plt.xlim([-0.5,1.5])\n",
    "plt.ylim([-0.5,2.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection, part 1:\n",
    "\n",
    "OK, that's pretty close to waht we expected!  We should have found slope = 1.1 and offset = -0.1.  But the noise in the problem will definitely give us some stochasticity in our results.  We should always make plots to validate that we're getting what we expect.  Let's do that here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
