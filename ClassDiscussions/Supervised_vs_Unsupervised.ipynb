{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['orange', 'cyan', 'cornflowerblue'])\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some data\n",
    "\n",
    "We want a problem that is not linearly separable to show how poorly a linear classification would work here.\n",
    "\n",
    "Note later that centering at zero is helpful for us.  We will make three classes, but you could add more or less if you wish by adjusting the variable K.  Feel free to add more dimensions, but it's harder to view beyond 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = 100 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "\n",
    "for j in range(K):\n",
    "  ix = range(N*j,N*(j+1))\n",
    "  r = np.linspace(0.0,1,N) # radius\n",
    "  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "  y[ix] = j\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([-1,1])\n",
    "plt.ylim([-1,1])\n",
    "#fig.savefig('spiral_raw.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Linear Classifier\n",
    "\n",
    "First let's try to make a linear classifier that separates each class from the others. With more convenient data, this would not be a crazy thing to do.  Our linear functions will be of the form:\n",
    "\n",
    "$f(X) = W^T\\cdot X +b$\n",
    "\n",
    "Here W is a matrix of weights and b is what's known as a bias variable.  You can just think of these as the slope and offset.  We also have the \"targets\" or truth labels for the data in each class: $y$.\n",
    "\n",
    "How do you decide if a linear classifier is doing a good job?  We have N classes, so we will make three lines.  Each line should be drawn between a given class and the N-1 other classes.  If you have a perfect separation, that means that you have $f_1(X)<y_1$ for all instances of class 1 and $f_1(X)>y_i$ for all other classes.  Or vice versa if you multiple everything by -1.\n",
    "\n",
    "In our case we have a 2D space and our function $f(X)$ will yield a 2D plane that will get increasingly positive in regions where the proper class exists and increasingly negative where the other classes exist.\n",
    "\n",
    "Thus we can turn this result into a probability for correct classification using what we call the SoftMax function:\n",
    "\n",
    "$S_i(X) = {e^{f_i(X)} \\over \\sum_j e^{f_j(X)}}$ \n",
    "\n",
    "This works well because if $f_i(x)$ is large, we map to a probability close to 1.0.  And, vice versa, if $f_i(X)$ is small and another $f_j(X)$ is large, we get a small probability.  And we find that $\\sum S_i=1$, which allows useful interpretations of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a Linear Classifier\n",
    "\n",
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,K)\n",
    "b = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "Niter = 200\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(Niter):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # fhat = W^TX +b\n",
    "    scores = np.dot(X, W) + b \n",
    "      \n",
    "    # compute the class probabilities\n",
    "    # this is softmax, converting to class probability\n",
    "    # prob1 = exp(score1)/{exp(score1)+exp(score2)+...}\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "      \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    # sometimes cross-entropy is called \"log loss\"\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "\n",
    "    # This is L2 regularization, see Lecture 5\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    \n",
    "    # Total loss is distance between the line and the data point\n",
    "    # plus the L2 reg loss\n",
    "    loss = data_loss + reg_loss\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(X.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "  \n",
    "    dW += reg*W # regularization gradient\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just to make it clear what we're doing, let's peek at the weights\n",
    "print(\"Weight matrix:\\n\",W)\n",
    "print(\"\\nBias matrix:\\n\",b)\n",
    "\n",
    "\n",
    "# evaluate training set accuracy\n",
    "scores = np.dot(X, W) + b\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('\\n training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's look to see what we got!\n",
    "# Our 2D weights/variables define planes for each class\n",
    "\n",
    "#make some x-values\n",
    "xyMnx = 1.2\n",
    "X_test = np.linspace(-1*xyMnx,xyMnx,N)\n",
    "Y_test = np.linspace(-1*xyMnx,xyMnx,N)\n",
    "\n",
    "Xt, Yt = np.meshgrid(X_test, Y_test)\n",
    "\n",
    "#2D plane separating class 1 from classes 2 and 3\n",
    "Z1 = Xt*W[0,0] + Yt*W[1,0] + b[0,0]\n",
    "\n",
    "#2D plane separating class 2 from classes 1 and 3\n",
    "Z2 = Xt*W[0,1] + Yt*W[1,1] + b[0,1]\n",
    "\n",
    "#2D plane separating class 3 from classes 1 and 2\n",
    "Z3 = Xt*W[0,2] + Yt*W[1,2] + b[0,2]\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "cp = plt.contourf(Xt, Yt, Z1,30, cmap=plt.cm.plasma, alpha=0.9)\n",
    "plt.colorbar(cp)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(-1*xyMnx,xyMnx)\n",
    "plt.ylim(-1*xyMnx,xyMnx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What we'd really like, though, is to classify space where\n",
    "# each class score has the highest probability.\n",
    "# This divides the space into Voronoi regions\n",
    "\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#fig.savefig('spiral_linear.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a one-layer neural network\n",
    "\n",
    "The linear classifier approach was fine, but it didn't correctly classify in more than ~50% of cases.  So we can ramp this up to a neural network.  We are still going to make linear functions of our input variables, $f(X) = W^TX+b$, but now we will use activation functions to introduce a non-linearity.  In this example we'll use \"Rectified Linear Units\", aka ReLU, as our activation function.  \n",
    "\n",
    "This NN will have one input layer (2 inputs), one hidden layer (100 neurons by default) and one output node that receives all of the activation function outputs. In the previous example we had $2\\times 3 + 3 =9$ weights.  Now we will have $2\\times 100 + 100=300$ weights for the inputs to the hidden layer and $100 \\times 3 + 3 = 303$ weights for the output node.  So we had better be able to improve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(10000):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # note that fhat is the same as before: fhat=W^T X + b\n",
    "    # but now we pass the linear sum into a ReLU activation function\n",
    "    # this provides a nonlinearity that wasn't there for the previous case\n",
    "    hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation\n",
    "\n",
    "    # this is the fully-connected output layer, just summing the output\n",
    "    # of the activation functions\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "    # compute the class probabilities\n",
    "    # this is softmax, converting to class probability\n",
    "    # prob1 = exp(score1)/{exp(score1)+exp(score2)+...}\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    \n",
    "    # this time we have two weight matrices, so we need to include\n",
    "    # both in the L2 regularization\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "  \n",
    "    if i % 1000 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    \n",
    "    # finally into W,b\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(X, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "Z = np.dot(np.maximum(0, np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b), W2) + b2\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#fig.savefig('spiral_net.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How well can we do with unsupervised learning?\n",
    "\n",
    "As an example of unsupervised learning, we can try adding in an unsupervised clustering algorithm.  Now we're going to borrow from scikit-learn for the k-Means algorithm to show how that works for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import  metrics\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "random_state = 170\n",
    " \n",
    "# Incorrect number of clusters\n",
    "y_pred2 = KMeans(n_clusters=2, random_state=random_state).fit_predict(X)\n",
    "    \n",
    "# Correct number of clusters\n",
    "y_pred3 = KMeans(n_clusters=3, random_state=random_state).fit_predict(X)\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred6 = KMeans(n_clusters=6, random_state=random_state).fit_predict(X)\n",
    "\n",
    "# Incorrect number of clusters\n",
    "y_pred15 = KMeans(n_clusters=10, random_state=random_state).fit_predict(X)\n",
    "\n",
    "#plt.subplot(221)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y_pred15,cmap=plt.cm.Spectral)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
