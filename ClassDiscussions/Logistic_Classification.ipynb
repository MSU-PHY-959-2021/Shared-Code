{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Create color maps\n",
    "cmap_list = ['orange', 'cyan', 'cornflowerblue']\n",
    "cmap_bold = ['darkorange', 'c', 'darkblue']\n",
    "cmap_light = ListedColormap(cmap_list)\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make some data\n",
    "\n",
    "We want a problem that is not linearly separable to show how poorly a linear classification would work here.\n",
    "\n",
    "Note later that centering at zero is helpful for us.  We will make three classes, but you could add more or less if you wish by adjusting the variable K.  Feel free to add more dimensions, but it's harder to view beyond 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "\n",
    "N = 150 # number of points per class\n",
    "D = 2 # dimensionality\n",
    "K = 3 # number of classes\n",
    "\n",
    "X = np.zeros((N*K,D))\n",
    "y = np.zeros(N*K, dtype='uint8')\n",
    "\n",
    "easy = 0  #set this to 1/True to have an easier data sample to train on\n",
    "slopeDiff = 0.0\n",
    "offsetDiff = 0.3\n",
    "\n",
    "min = -1\n",
    "max = 1\n",
    "if easy:\n",
    "    min = -0.5\n",
    "    max = 2\n",
    "    \n",
    "for j in range(K):\n",
    "    ix = range(N*j,N*(j+1))\n",
    "    r = np.linspace(0.0,1,N) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n",
    "    xx = r*np.sin(t)\n",
    "    yy = r*np.cos(t)\n",
    "    if easy:\n",
    "        xx = np.linspace(0,1,N)\n",
    "        yy = (0.5+0.5*slopeDiff*(j+1))*xx + np.random.randn(N)*(0.05+j*0.05) + j*offsetDiff\n",
    "    \n",
    "    X[ix] = np.c_[xx,yy]\n",
    "    y[ix] = j\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim([min,max])\n",
    "plt.ylim([min,max])\n",
    "#fig.savefig('spiral_raw.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a neural network with one hidden layer\n",
    "\n",
    "The linear classifier approach was fine, but it didn't correctly classify in more than ~50% of cases.  So we can ramp this up to a neural network.  We are still going to make linear functions of our input variables, $f(X) = W^TX+b$, but now we will use activation functions to introduce a non-linearity.  In this example we'll use \"Rectified Linear Units\", aka ReLU, as our activation function.  \n",
    "\n",
    "This NN will have one input layer (2 inputs), one hidden layer (100 neurons by default) and one output node that receives all of the activation function outputs. In the previous example we had $2\\times 3 + 3 =9$ weights.  Now we will have $2\\times 100 + 100=300$ weights for the inputs to the hidden layer and $100 \\times 3 + 3 = 303$ weights for the output node.  So we had better be able to improve!\n",
    "\n",
    "But we need to study some activation functions first.  Let's see what we can find out about the following options:\n",
    "  * Rectified Logical Unit (ReLU)\n",
    "  * Sigmoid\n",
    "  * SoftPlus\n",
    "  * Leaky ReLU\n",
    "  * Exponential ReLU (ELU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our NN activation function here\n",
    "def activation(i, input):\n",
    "\n",
    "    if i == 0:\n",
    "        # Rectified linear unit (ReLU) activation\n",
    "        output = np.maximum(0,input)\n",
    "    elif i == 1:\n",
    "        #Sigmoid\n",
    "        output =  1/(1+np.exp(-1*input))\n",
    "    elif i == 2:\n",
    "        # SoftPlus\n",
    "        output = np.log(1+np.exp(input))\n",
    "    elif i == 3:\n",
    "        # Leaky ReLU\n",
    "        output = np.maximum(0.1*input,0.9*input)\n",
    "    elif i == 4:\n",
    "        # Exponential ReLU\n",
    "        output = np.where(input>0,input,np.exp(input)-1)\n",
    "    else:\n",
    "        return input\n",
    "    \n",
    "    return output\n",
    "\n",
    "ix = np.linspace(-4,4,200)\n",
    "\n",
    "plt.plot(ix,activation(0,ix),color=\"red\",label=\"ReLU\")\n",
    "plt.plot(ix,activation(1,ix),color=\"blue\",label=\"Sigmoid\")\n",
    "plt.plot(ix,activation(2,ix),color=\"green\",label=\"SoftPlus\")\n",
    "plt.plot(ix,activation(3,ix),color=\"magenta\",label=\"Leaky ReLU\")\n",
    "plt.plot(ix,activation(4,ix),color=\"orange\",label=\"ELU\")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "\n",
    "W2 = 0.01 * np.random.randn(h,K)\n",
    "b2 = np.zeros((1,K))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "iActivate = 0\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = X.shape[0]\n",
    "for i in range(10001):\n",
    "  \n",
    "    # evaluate class scores, [N x K]\n",
    "    # note that fhat is the same as before: fhat=W^T X + b\n",
    "    fhat = np.dot(X, W) + b\n",
    "    \n",
    "    # but now we pass the linear sum into an activation function\n",
    "    # this provides a nonlinearity that wasn't there for the previous case\n",
    "    hidden_layer = activation(iActivate,fhat)\n",
    "    \n",
    "    # this is the fully-connected output layer, just summing the output\n",
    "    # of the activation functions\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "  \n",
    "    # compute the class probabilities\n",
    "    # this is softmax, converting to class probability\n",
    "    # prob1 = exp(score1)/{exp(score1)+exp(score2)+...}\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "  \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    corect_logprobs = -np.log(probs[range(num_examples),y])\n",
    "    data_loss = np.sum(corect_logprobs)/num_examples\n",
    "    \n",
    "    # this time we have two weight matrices, so we need to include\n",
    "    # both in the L2 regularization\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "  \n",
    "    if i % 1000 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "  \n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),y] -= 1\n",
    "    dscores /= num_examples\n",
    "  \n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    \n",
    "    # finally into W,b\n",
    "    dW = np.dot(X.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "  \n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "  \n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = activation(iActivate,fhat)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting classifier\n",
    "h = 0.02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                     np.arange(y_min, y_max, h))\n",
    "iArg = np.dot(np.c_[xx.ravel(), yy.ravel()], W) + b\n",
    "Z = np.dot(activation(iActivate,iArg), W2) + b2\n",
    "\n",
    "Z = np.argmax(Z, axis=1)\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "#fig.savefig('spiral_net.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have N x 3 and need the first column split by \n",
    "# If we pick class = 2, that gives us \"blue vs the rest\"\n",
    "iClass = 2\n",
    "Xplt = np.zeros((N,3))\n",
    "Xplt[:,0] = scores[:N,iClass]\n",
    "Xplt[:,1] = scores[N:2*N,iClass]\n",
    "Xplt[:,2] = scores[2*N:3*N,iClass]\n",
    "\n",
    "plt.hist(Xplt,30,color=cmap_list)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
