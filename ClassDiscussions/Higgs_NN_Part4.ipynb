{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bisect\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "#plt.style.use(\"seaborn\")\n",
    "plt.rcParams.update(plt.rcParamsDefault)\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "# good behavior for notebooks!\n",
    "sns.set(context='notebook' , color_codes=True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-oasis",
   "metadata": {},
   "source": [
    "# Finding Higgs Bosons with PyTorch!!\n",
    "\n",
    "This notebook builds on our previous notebooks in which we were studying the Higgs boson Kaggle challenge.  We've now seen the `scikit-learn` implementation (super simple black box) and we've seen the `Keras + TensorFlow` implementation (simple, but intuitive control of the NN).\n",
    "\n",
    "This time, we're going to try out PyTorch to give you a feeling for how `PyTorch` connects you much more closely to the tools that build up the NN.  There are a few things specific to `PyTorch` to note, but otherwise we will have a very similar overall picture of what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-philip",
   "metadata": {},
   "source": [
    "# Load the data sample\n",
    "\n",
    "The first thing to do here is load the data sample.  We will use the `pandas` `dataframe` class, as it's well suited to this task.  For the training sample, we will use the first 350k samples.  The remaining samples will be for validation and evaluation of independent AMS scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-kenya",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the first 350k samples\n",
    "dataset=pd.read_csv('higgs-boson/atlas-higgs-challenge-2014-v2.csv',nrows=350000)\n",
    "\n",
    "# summarize the data\n",
    "\n",
    "# shape\n",
    "print(\"Shape:\\n\",dataset.shape, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-deputy",
   "metadata": {},
   "source": [
    "# Now groom the data sample\n",
    "\n",
    "We need prepare a subset of the sample on which to train.  We will remove all of the variables that should not be used in the training and retain the rest.  In addition, we'll keep a copy of the labels and weights for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flying-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a new dataframe without some of the features\n",
    "X = dataset.drop([ 'Label','EventId','Weight','KaggleSet','KaggleWeight'],axis=1,inplace=False)\n",
    "weights = dataset['Weight'].values\n",
    "\n",
    "# We'll find it useful to have the dataframe column names\n",
    "colNames = X.columns\n",
    "\n",
    "# Two forms of the \n",
    "y = pd.get_dummies(dataset.Label)\n",
    "yt = y.values[:,0]\n",
    "\n",
    "# Build selectors to separate our data in to signal and background\n",
    "sSelector = np.array(yt == 0)\n",
    "bSelector = np.array(yt == 1)\n",
    "\n",
    "# Calculate what's in our samples\n",
    "sumWeights = np.sum(weights)\n",
    "sumSWeights = np.sum(weights[sSelector])\n",
    "sumBWeights = np.sum(weights[bSelector])\n",
    "print(\"Total Events: {}, Signal: {}, Background: {}\".format(sumWeights,sumSWeights,sumBWeights))\n",
    "\n",
    "# Normalize weights for signal to match the background in magnitude\n",
    "nweights = np.copy(weights)\n",
    "nweights[sSelector] *= sumBWeights/sumSWeights  \n",
    "\n",
    "# Perform standard scaling!\n",
    "scaler = StandardScaler()  \n",
    "# Don't cheat - fit only on training data\n",
    "scaler.fit(X.values)  \n",
    "Xplot = scaler.transform(X.values)\n",
    "\n",
    "# Recall that PyTorch uses Tensors for everything!\n",
    "# Thus we need to convert from numpy arrays to Tensors here.\n",
    "tX0 = torch.from_numpy(Xplot)\n",
    "ty0 = torch.from_numpy(y.values[:,1])\n",
    "\n",
    "# Pytorch likes float for the design matrix and long for targets\n",
    "tX = tX0.to(torch.float32)\n",
    "ty = ty0.to(torch.long)\n",
    "\n",
    "print(tX.shape)\n",
    "print(ty.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-identifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRange(sig, bkg):\n",
    "    ret = np.zeros(2)\n",
    "    amin = np.min(sig)\n",
    "    ret[0] = amin\n",
    "    if amin <-998:\n",
    "        ret[0] = 0\n",
    "    vals, bins = np.histogram(sig,bins=100)\n",
    "    vals2, bins2 = np.histogram(bkg,bins=100)\n",
    "\n",
    "    ret[1] = np.maximum(np.max(sig),np.max(bkg))\n",
    "\n",
    "    idx = 0\n",
    "    for val in vals:\n",
    "        if val > 2:\n",
    "            ret[1] = bins[idx+1]\n",
    "        if vals2[idx] > 2:\n",
    "            ret[1] = bins2[idx+1]\n",
    "        idx += 1\n",
    "        \n",
    "    if ret[1] < ret[0]:\n",
    "        ret[1] = np.maximum(np.max(sig),np.max(bkg))\n",
    "    \n",
    "    if np.abs(1.0-ret[1])<0.1:\n",
    "        ret[1] = 1.0\n",
    "    \n",
    "    return ret\n",
    "\n",
    "def plotHisto(sig,bkg,label,ax, doNorm=True, s_weight=None, b_weight=None):\n",
    "    trange = getRange(sig,bkg)\n",
    "    \n",
    "    sweight = None\n",
    "    bweight = None\n",
    "    if doNorm == True:\n",
    "        sweight = s_weight\n",
    "        bweight = b_weight\n",
    "    \n",
    "    sVals, edges = np.histogram(sig,bins=100,range=trange,weights=sweight)\n",
    "    bVals, edges = np.histogram(bkg,bins=100,range=trange,weights=bweight)\n",
    "    xvals = np.linspace(trange[0],trange[1],100)\n",
    "    \n",
    "    if doNorm:\n",
    "        ax.plot(xvals,sVals/np.sum(sVals),label=\"Signal\")\n",
    "        ax.plot(xvals,bVals/np.sum(bVals),label=\"Bkgd\")\n",
    "        ax.set_title(\"{}, Normalized\".format(label))\n",
    "    else:\n",
    "        ax.plot(xvals,sVals,label=\"Signal\")\n",
    "        ax.plot(xvals,bVals,label=\"Bkgd\")\n",
    "        ax.set_title(\"{}, Unnormalized\".format(label))\n",
    "    ax.set_ylim(bottom=0)\n",
    "    ax.legend()\n",
    "\n",
    "def plotNNresults(predictions,sSelector,bSelector,weights):\n",
    "    fig, axs = plt.subplots(1,2, figsize=(14,6))\n",
    "    plotHisto(predictions[sSelector,1],predictions[bSelector,1],\"Neural Nework\",\n",
    "              axs[0],doNorm=False)\n",
    "    plotHisto(predictions[sSelector,1],predictions[bSelector,1],\"Neural Nework\",\n",
    "              axs[1],doNorm=True,b_weight=weights[bSelector],s_weight=weights[sSelector])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "universal-evanescence",
   "metadata": {},
   "source": [
    "# Build a Dataset and DataLoader\n",
    "\n",
    "This step was not necessary for our `Keras` or `scikit-learn` models, but note how the specific setup of a `Dataset` and `DataLoader` gives us options to control details of how the data is accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-bacon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derive a simple dataset from the torch.util.data.Dataset class\n",
    "class higgsDataSet(data.Dataset):\n",
    "    def __init__(self, X, y, *args, **kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        # X = design matrix\n",
    "        # y = targets\n",
    "        self.input = X\n",
    "        self.target = y\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        ix = self.input[idx]\n",
    "        iy = self.target[idx]\n",
    "        return ix, iy\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "\n",
    "\n",
    "# Pass our Tensors to our dataset class\n",
    "higgsData = higgsDataSet(tX,ty)\n",
    "\n",
    "# Create a dataloader to allow easy mini-batches and shuffling.\n",
    "higgsLoader = data.DataLoader(dataset = higgsData, batch_size=500, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-district",
   "metadata": {},
   "source": [
    "# Build a neural network\n",
    "\n",
    "Let's build up a `PyTorch` neural network.  First we define the model, then we need to build our training program.  Please note that we're using mini-batch SGD, so we're going to caculate our average loss over each epoch.  If you focus on the loss from each batch, it will not make sense as it will fluctuate a lot.  Also, note that we're going to clear the gradients after each batch.  This gives us finer control over what happens with the gradients that are being accumulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "according-compression",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Hyper parameters for our NN structure\n",
    "l1_nodes = 150\n",
    "l2_nodes = 50\n",
    "\n",
    "#Define a NN model.  Note that we first define the layers we want and\n",
    "# then stitch them together in the forward pass argument.\n",
    "class higgsModel(nn.Module): \n",
    "    def __init__(self, inDim):\n",
    "        super().__init__()\n",
    "        # Here we build the layers we want to use later on\n",
    "        # Note that we want to define dropout here, so that it does the right thing\n",
    "        # between training and evaluation!\n",
    "        self.fc1 = nn.Linear(in_features = inDim, out_features = l1_nodes, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features = l1_nodes, out_features = l2_nodes, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features = l2_nodes, out_features = 2, bias=True)\n",
    "\n",
    "        # By defining our helper functions here, we get better behavior\n",
    "        # ReLU doesn't need to be instantiated every forward pass!\n",
    "        self.relu = nn.ReLU()\n",
    "        # On top of efficiency, defining dropout here allows it to be \"ignored\" in\n",
    "        # the eval step.\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x)) \n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x)) \n",
    "        x = self.dropout(x)\n",
    "        # I want to use softmax for the output, but the PyTorch cross entropy loss already\n",
    "        # uses this.  So I leave softmax out of the forward pass, but will use softmax when\n",
    "        # evaluating.\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# We will control every step of our model training here.\n",
    "def trainModel(loader, inDim, cWeight = None, verbose=False, sweights=None):\n",
    "    model = higgsModel(inDim)\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    # Cross-entropy for PyTorch is log_softmax + log-loss\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=cWeight)\n",
    "\n",
    "    # Adam is always a good choice to start.\n",
    "    # Weight decay = L2 regularization strength, default = 0\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=0.001,weight_decay=1e-5)\n",
    "\n",
    "    nIter = 50\n",
    "    for t in range(nIter):\n",
    "        print('')\n",
    "\n",
    "        # Containers to track average loss and accuracy\n",
    "        iLoss = 0\n",
    "        iAcc = 0\n",
    "        for bIdx, (iBatch, iTarget) in enumerate(loader):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = model(iBatch)\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, iTarget)\n",
    "\n",
    "            # Here we add L1 regularization \"by hand\"\n",
    "            l1_lambda = 1e-6\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "            # Don't forget to add to the loss\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "    \n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the variables it will update (which are the learnable\n",
    "            # weights of the model). This is because by default, gradients are\n",
    "            # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "            # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model\n",
    "            # parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its\n",
    "            # parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            # Here I'm summing up my loss over my batches and we'll report the\n",
    "            # average loss and accuracy with some periodicity next.\n",
    "            iLoss += loss.item()\n",
    "            iAcc += accuracy_score(iTarget.detach().numpy(), np.argmax(y_pred.detach().numpy(), axis=1))\n",
    "            \n",
    "            # report progress\n",
    "            if bIdx % 10 == 0:\n",
    "                print(f'Training epoch {t} [{bIdx*len(iBatch)}/{len(loader.dataset)}]: \\tLoss = {iLoss/(bIdx+1):.5f}, Accuracy: {iAcc/(bIdx+1):.4f}',end='\\r')\n",
    "    \n",
    "    # return the model so we can use it!\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-oracle",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We have 2x more background examples than signal examples, so we can account for this.\n",
    "class_weight = torch.tensor([1,2],dtype=torch.float32)\n",
    "\n",
    "# Create our model and train it!\n",
    "modelAll = trainModel(higgsLoader,tX.shape[1],cWeight=class_weight, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-wheat",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Very important!!  When we make our predictions, we have to switch our NN over to\n",
    "# using the evaluation state.  This is because we're using dropout layers and do not\n",
    "# want to have nodes randomly dropped for this step.\n",
    "predictionsAll = func.softmax(modelAll.eval()(tX),dim=1)\n",
    "\n",
    "# Make a plot!\n",
    "plotNNresults(predictionsAll.detach().numpy(),sSelector,bSelector,weights) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serious-pennsylvania",
   "metadata": {},
   "source": [
    "# Finally, let's check to see how we did for our challenge\n",
    "\n",
    "The challenge was \"judged\" based on the Approximate Mean Significance (AMS) score.  Let's just quickly check how we did on our training data.  This is an over-estimate because we trained on this data, but it will give us an idea of how we did!  Remember the best work in the Higgs ML challenge achieved an AMS score of ~ 3.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-balloon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ams(s,b):\n",
    "    from math import sqrt,log\n",
    "    if b==0:\n",
    "        return 0\n",
    "    \n",
    "    return sqrt(2*((s+b+10)*log(1+float(s)/(b+10))-s))\n",
    "\n",
    "def plotAMS(Xa,ya,wa,sel=None):\n",
    "    yp = ya\n",
    "    wp = wa\n",
    "    if sel is not None:\n",
    "        yp = ya[sel]\n",
    "        wp = wa[sel]\n",
    "        \n",
    "    # order them \n",
    "    sSelector = np.array(yp == 0)\n",
    "    bSelector = np.array(yp == 1)\n",
    "    \n",
    "    permute = Xa.argsort()\n",
    "    sSelector = sSelector[permute]\n",
    "    bSelector = bSelector[permute]\n",
    "    Xa = Xa[permute]\n",
    "    wp = wp[permute]\n",
    "\n",
    "    # pick out only the signal values\n",
    "    sVals = Xa[sSelector]\n",
    "    sigW = wp[sSelector]\n",
    "\n",
    "    # pick out only the background values\n",
    "    bVals = Xa[bSelector]\n",
    "    bkgW = wp[bSelector]\n",
    "\n",
    "    # make arrays\n",
    "    xvals = np.linspace(0,1,1000)\n",
    "    amsV = np.linspace(0,1,1000)\n",
    "    bkgFrac = np.linspace(0,1,1000)\n",
    "    aIdx = 0\n",
    "\n",
    "    # We are going to organize by background rejection fraction,\n",
    "    # so we need to find those values\n",
    "    for xx in xvals:\n",
    "        idxS = bisect.bisect_left(sVals,xx)\n",
    "        idxB = bisect.bisect_left(bVals,xx)\n",
    "        amsV[aIdx] = ams(np.sum(sigW[idxS:]),np.sum(bkgW[idxB:]))\n",
    "        bkgFrac[aIdx] = 1-np.sum(bkgW[idxB:])/np.sum(bkgW)\n",
    "        aIdx += 1\n",
    "\n",
    "    permute = bkgFrac.argsort()\n",
    "    bkgFrac = bkgFrac[permute]\n",
    "    amsV = amsV[permute]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = (12,8))\n",
    "    ax.plot(bkgFrac*100,amsV)\n",
    "    ax.set_xlabel(\"Background rejection (%)\")\n",
    "    ax.set_ylabel(\"AMS Score\")\n",
    "    plt.show()\n",
    "    print(\"Maximum AMS Value: \",np.max(amsV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-cardiff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# read the first last ~468k samples\n",
    "datasetVal=pd.read_csv('higgs-boson/atlas-higgs-challenge-2014-v2.csv',skiprows = range(1, 350000))\n",
    "#datasetVal=pd.read_csv('higgs-boson/training.csv')\n",
    "\n",
    "\n",
    "# shape\n",
    "print(\"Shape:\\n\",datasetVal.shape, \"\\n\")\n",
    "\n",
    "# Make a new dataframe without some of the features\n",
    "Xval = datasetVal.drop([ 'Label','EventId','Weight','KaggleSet','KaggleWeight'],axis=1,inplace=False)\n",
    "#Xval = datasetVal.drop([ 'Label','EventId','Weight'],axis=1,inplace=False)\n",
    "weightsVal = datasetVal['Weight'].values\n",
    "\n",
    "# Perform standard scaling!\n",
    "XplotV = scaler.transform(Xval.values)  \n",
    "\n",
    "# Two forms of the labels\n",
    "yv = pd.get_dummies(datasetVal.Label)\n",
    "yVal = yv.values[:,0]\n",
    "\n",
    "# Don't for get to Tensor-ize!\n",
    "tX0 = torch.from_numpy(XplotV)\n",
    "ty0 = torch.from_numpy(yv.values[:,1])\n",
    "\n",
    "tXval = tX0.to(torch.float32)\n",
    "tyval = ty0.to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tender-bridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate our new data with the NN\n",
    "predictionsAll = func.softmax(modelAll.eval()(tXval),dim=1)\n",
    "\n",
    "# Plot the AMS scores\n",
    "plotAMS(predictionsAll.detach().numpy()[:,1],yVal,weightsVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
